# -*- coding: utf-8 -*-
"""Pneumonia_Detection(23cs081).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k6slowNzcE2pb66x4AQoCMa8RSwjEXZi
"""

from google.colab import files
files.upload()  # This will open a file upload dialog

import os
os.makedirs("/root/.kaggle", exist_ok=True)

import shutil
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

os.chmod("/root/.kaggle/kaggle.json", 600)

!pip install -q kaggle

!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia

!unzip chest-xray-pneumonia.zip -d /content/

!ls /content/chest_xray

import os
train_dir = '/content/chest_xray/train'
print(os.listdir(train_dir))

import os

base_dir = '/content/chest_xray'
print(os.listdir(base_dir))  # Should show ['train', 'test', 'val']
print(os.listdir(os.path.join(base_dir, 'train')))  # Should show ['NORMAL', 'PNEUMONIA']

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

import os
import numpy as np
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Directories
train_dir = '/content/chest_xray/train'
val_dir = '/content/chest_xray/val'
test_dir = '/content/chest_xray/test'

# Function to load images and labels
def load_data(directory, target_size=(150, 150)):
    images, labels = [], []
    for label, folder in enumerate(['NORMAL', 'PNEUMONIA']):
        path = os.path.join(directory, folder)
        for img_file in os.listdir(path):
            img_path = os.path.join(path, img_file)
            img = load_img(img_path, target_size=target_size)
            img_array = img_to_array(img)/255.0
            images.append(img_array)
            labels.append(label)
    return np.array(images), np.array(labels)

# Load datasets
X_train, y_train = load_data(train_dir)
X_val, y_val = load_data(val_dir)
X_test, y_test = load_data(test_dir)

print(X_train.shape, y_train.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Simple CNN
cnn_model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
cnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

test_loss, test_acc = cnn_model.evaluate(X_test, y_test)
print(f"CNN Test Accuracy: {test_acc*100:.2f}%")

for i, layer in enumerate(cnn_model.layers):
    print(i, layer.name, layer.__class__.__name__)

# Make sure the model is built
cnn_model.build((None, 150, 150, 3))
cnn_model.summary()

# Example
cnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=32)

def get_gradcam(model, img_array):
    # Find last Conv2D layer automatically
    for layer in reversed(model.layers):
        if 'conv' in layer.name:
            last_conv_layer = layer.name
            break

    grad_model = tf.keras.models.Model([model.inputs],
                                       [model.get_layer(last_conv_layer).output, model.output])

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        loss = predictions[:, 0]

    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)

    return heatmap.numpy()

from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

inputs = Input(shape=(150, 150, 3))
x = Conv2D(32, (3,3), activation='relu')(inputs)
x = MaxPooling2D(2,2)(x)
x = Conv2D(64, (3,3), activation='relu')(x)
x = MaxPooling2D(2,2)(x)
x = Conv2D(128, (3,3), activation='relu')(x)
x = MaxPooling2D(2,2)(x)
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(1, activation='sigmoid')(x)

cnn_model = Model(inputs=inputs, outputs=outputs)
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

cnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

import tensorflow as tf
import numpy as np

def get_gradcam(model, img_array):
    # Automatically pick last Conv2D layer
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            last_conv_layer = layer.name
            break

    grad_model = tf.keras.models.Model(model.inputs,
                                   [model.get_layer(last_conv_layer).output, model.output])


    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        loss = predictions[:, 0]

    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)

    return heatmap  # instead of heatmap.numpy()

import matplotlib.pyplot as plt
import cv2

img = tf.convert_to_tensor(X_test[0][np.newaxis, ...], dtype=tf.float32)

heatmap = get_gradcam(cnn_model, img)

plt.imshow(X_test[0])
plt.imshow(cv2.resize(heatmap, (150,150)), cmap='jet', alpha=0.5)
plt.axis('off')
plt.show()

import numpy as np

def predict_proba_cnn(images):
    """
    CNN outputs sigmoid (0-1). LIME expects 2-class probabilities.
    Returns shape (num_samples, 2): [Normal, Pneumonia]
    """
    probs = cnn_model.predict(images)
    probs_2class = np.zeros((probs.shape[0], 2))
    probs_2class[:, 0] = 1 - probs[:, 0]  # Normal
    probs_2class[:, 1] = probs[:, 0]      # Pneumonia
    return probs_2class

img = X_test[0]  # shape (150, 150, 3)
pred_label = np.argmax(predict_proba_cnn(img[np.newaxis, ...]))

!pip install lime

from lime import lime_image
from skimage.color import label2rgb

explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img.astype('double'),
    classifier_fn=predict_proba_cnn,
    top_labels=2,
    hide_color=0,
    num_samples=1000
)

# Get image and mask
temp, mask = explanation.get_image_and_mask(
    label=pred_label,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

img_boundry = label2rgb(mask, img, bg_label=0)
import matplotlib.pyplot as plt
plt.imshow(img_boundry)
plt.axis('off')
plt.show()

import cv2

# Grad-CAM
gradcam_overlay = cv2.resize(heatmap, (150,150))
cv2.imwrite("gradcam_overlay.png", (gradcam_overlay * 255).astype(np.uint8))

# LIME
plt.imsave("lime_explanation.png", img_boundry)

from sklearn.metrics import confusion_matrix, classification_report

y_pred = (cnn_model.predict(X_test) > 0.5).astype(int)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=['Normal','Pneumonia']))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Predictions
y_pred = (cnn_model.predict(X_test) > 0.5).astype(int)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
            xticklabels=['Normal', 'Pneumonia'],
            yticklabels=['Normal', 'Pneumonia'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

history = cnn_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32
)

# Accuracy
plt.figure(figsize=(6,4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.show()

# Loss
plt.figure(figsize=(6,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.show()

import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix

# 1) Get validation probabilities once (no training)
val_probs = model.predict(X_val, verbose=0).ravel()

# 2) Search for the best threshold for accuracy on VAL
thresholds = np.linspace(0.1, 0.9, 81)
best_thr, best_acc = 0.5, 0
for t in thresholds:
    val_pred = (val_probs >= t).astype(int)
    acc = accuracy_score(y_val, val_pred)
    if acc > best_acc:
        best_acc, best_thr = acc, t

print(f"Best threshold on VAL: {best_thr:.3f}  |  Val accuracy: {best_acc:.4f}")

# 3) Apply this threshold on TEST
test_probs = model.predict(X_test, verbose=0).ravel()
y_pred_thr = (test_probs >= best_thr).astype(int)
cm = confusion_matrix(y_test, y_pred_thr)
acc = accuracy_score(y_test, y_pred_thr)
print(f"Test accuracy @ {best_thr:.3f}: {acc:.4f}")
print(cm)

import tensorflow as tf
import numpy as np

def tta_batch_predict(model, X, rotations_deg=(0, 10, -10), hflip=True):
    preds = []

    # Original
    preds.append(model.predict(X, verbose=0).ravel())

    # Horizontal flip (reasonable for CXR)
    if hflip:
        X_h = tf.image.flip_left_right(X).numpy()
        preds.append(model.predict(X_h, verbose=0).ravel())

    # Small rotations
    for deg in rotations_deg:
        if deg == 0:
            continue
        from scipy.ndimage import rotate

def rotate_batch(X, degrees):
    X_rot = []
    for img in X:
        img_r = rotate(img, degrees, reshape=False, mode='nearest')
        X_rot.append(img_r)
    return np.array(X_rot)

def tta_batch_predict(model, X, rotations_deg=(10, -10), hflip=True):
    preds = []
    # Original
    preds.append(model.predict(X, verbose=0).ravel())
    # Horizontal flip
    if hflip:
        X_h = tf.image.flip_left_right(X).numpy()
        preds.append(model.predict(X_h, verbose=0).ravel())
    # Rotations (using scipy)
    for deg in rotations_deg:
        X_r = rotate_batch(X, deg)
        preds.append(model.predict(X_r, verbose=0).ravel())
    # Average predictions
    return np.mean(np.stack(preds, axis=0), axis=0)

import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.ndimage import rotate

# ----------------------------
# Rotation helper (using SciPy)
# ----------------------------
def rotate_batch(X, degrees):
    X_rot = []
    for img in X:
        img_r = rotate(img, degrees, reshape=False, mode='nearest')
        X_rot.append(img_r)
    return np.array(X_rot)

# ----------------------------
# TTA prediction function
# ----------------------------
def tta_batch_predict(model, X, rotations_deg=(10, -10), hflip=True):
    preds = []
    # original
    preds.append(model.predict(X, verbose=0).ravel())
    # horizontal flip
    if hflip:
        X_h = tf.image.flip_left_right(X).numpy()
        preds.append(model.predict(X_h, verbose=0).ravel())
    # rotations
    for deg in rotations_deg:
        X_r = rotate_batch(X, deg)
        preds.append(model.predict(X_r, verbose=0).ravel())
    # average predictions
    return np.mean(np.stack(preds, axis=0), axis=0)

# ----------------------------
# Step 1: Run TTA on val + test
# ----------------------------
val_probs_tta = tta_batch_predict(model, X_val)
test_probs_tta = tta_batch_predict(model, X_test)

# ----------------------------
# Step 2: Threshold tuning
# ----------------------------
thresholds = np.linspace(0.1, 0.9, 81)
best_thr, best_acc = 0.5, 0
for t in thresholds:
    val_pred = (val_probs_tta >= t).astype(int)
    acc = accuracy_score(y_val, val_pred)
    if acc > best_acc:
        best_acc, best_thr = acc, t

print(f"TTA best threshold on VAL: {best_thr:.3f} | Val acc: {best_acc:.4f}")

# ----------------------------
# Step 3: Final Test Evaluation
# ----------------------------
y_pred_tta = (test_probs_tta >= best_thr).astype(int)
test_acc = accuracy_score(y_test, y_pred_tta)
print(f"Test acc (TTA): {test_acc:.4f}")
print("Confusion Matrix (values):\n", confusion_matrix(y_test, y_pred_tta))

# ----------------------------
# Step 4: Confusion Matrix Heatmap
# ----------------------------
cm = confusion_matrix(y_test, y_pred_tta)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Normal", "Pneumonia"],
            yticklabels=["Normal", "Pneumonia"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (with TTA)")
plt.show()

# ----------------------------
# Step 5: ROC Curve
# ----------------------------
fpr, tpr, thresholds = roc_curve(y_test, test_probs_tta)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f"ROC curve (AUC = {roc_auc:.2f})")
plt.plot([0,1], [0,1], color='gray', linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve (with TTA)")
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Accuracy
test_acc = accuracy_score(y_test, y_pred_tta)
print(f"Test Accuracy: {test_acc*100:.2f}%")

# Classification report (Precision, Recall, F1)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_tta, target_names=["Normal", "Pneumonia"]))

# Confusion matrix (numbers only)
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_tta))